{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [],
   "name": "FBMFOR_Chemometrics_Analysis.ipynb"
  }
 },
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "---\ntitle: \"Chemometric Analysis for Food Fraud Detection\"\nformat:\n  html:\n    code-fold: false\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udd2c Chemometric Analysis Notebook\n## FBMFOR \u2014 Food Fraud Analysis\n**MSc Food Technology and Quality Assurance** | University of Reading\n\nThis notebook provides a complete chemometric analysis pipeline for spectroscopic data (NMR, FTIR) used in food fraud detection. You will work through:\n\n1. **Data upload and exploration** \u2014 load your spectral data and metadata\n2. **Preprocessing** \u2014 baseline correction, normalisation, and derivatives\n3. **Principal Component Analysis (PCA)** \u2014 unsupervised exploration\n4. **Hierarchical Cluster Analysis (HCA)** \u2014 sample grouping\n5. **PLS-DA** \u2014 supervised classification\n6. **OPLS-DA** \u2014 orthogonal signal correction for enhanced discrimination\n\nEach section includes explanatory text describing the *what* and *why* of each step. You are encouraged to modify parameters and observe the effects.\n\n> **Data format:** CSV files with samples as rows and spectral variables (wavenumbers/chemical shifts) as columns. The first column should contain sample IDs, and a separate metadata CSV maps sample IDs to class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n\nRun this cell to install and import all required packages. On Google Colab, most are pre-installed; only `plotly` may need updating."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# INSTALL & IMPORT\n# ============================================================\n# Uncomment the next line if plotly is outdated on your Colab instance\n# !pip install --upgrade plotly\n\n# --- Core numerical and data libraries ---\nimport numpy as np                  # Array operations, linear algebra\nimport pandas as pd                 # DataFrames for tabular data\n\n# --- Plotting ---\nimport matplotlib.pyplot as plt     # Static plots (used for HCA dendrogram)\nimport plotly.express as px         # High-level interactive plots\nimport plotly.graph_objects as go   # Low-level interactive plot control\nfrom plotly.subplots import make_subplots  # Multi-panel interactive figures\n\n# --- Signal processing and clustering ---\nfrom scipy import signal, sparse    # Savitzky-Golay filter, sparse matrices for ALS\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster  # HCA\nfrom scipy.spatial.distance import pdist    # Pairwise distance computation\nfrom scipy.stats import f as f_dist         # F-distribution for Hotelling's T\u00b2\n\n# --- Machine learning ---\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder  # Data scaling, label encoding\nfrom sklearn.decomposition import PCA                           # Principal Component Analysis\nfrom sklearn.cross_decomposition import PLSRegression           # PLS (used for PLS-DA and OPLS-DA)\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold  # Cross-validation\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score  # Evaluation\n\n# --- Utility ---\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # Suppress convergence warnings from PLS\n\nprint(\"\u2705 All packages imported successfully.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demo Dataset: Simulated Olive Oil FTIR\n\nThis section generates a **realistic simulated FTIR dataset** for olive oil authentication. The dataset mimics mid-infrared spectra (4000\u2013400 cm\u207b\u00b9) for three classes:\n\n- **Extra Virgin Olive Oil (EVOO)** \u2014 authentic\n- **Refined Olive Oil (ROO)** \u2014 lower grade, sometimes mislabelled\n- **Adulterated** \u2014 EVOO blended with hazelnut or sunflower oil\n\nKey spectral differences are introduced in regions known to be diagnostic:\n- ~1745 cm\u207b\u00b9 (carbonyl C=O stretch \u2014 ester linkage, differs with fatty acid profile)\n- ~2925 and ~2854 cm\u207b\u00b9 (C\u2013H stretching \u2014 related to chain length and saturation)\n- ~1160 cm\u207b\u00b9 (C\u2013O stretch \u2014 differs with triglyceride composition)\n- ~3005 cm\u207b\u00b9 (=C\u2013H stretch \u2014 indicator of unsaturation)\n\nThe simulation includes realistic sources of variation found in real laboratory data:\n- **Peak position shifts** (\u00b12\u20133 cm\u207b\u00b9) from instrument calibration and temperature drift\n- **Intensity variation** (\u00b115%) from natural compositional differences within each class\n- **Sample-specific baselines** from different pathlengths and sample preparation\n- **Three outlier samples** with intentional anomalies (borderline quality, ATR artefact, subtle adulteration) \u2014 see if you can find them with Hotelling\u2019s T\u00b2!\n\n> **Skip this section** if you want to upload your own data \u2014 proceed to Section 3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# GENERATE SIMULATED OLIVE OIL FTIR DATA\n# ============================================================\n# Set the global random seed for reproducibility. All students\n# running this notebook will get identical simulated spectra.\nnp.random.seed(11088)\n\n# --- Define the wavenumber axis ---\n# Mid-infrared range from 4000 to 400 cm-1, with 1800 points\n# (approx. 2 cm-1 resolution, typical for benchtop FTIR)\nwavenumbers = np.linspace(4000, 400, 1800)\n\n\ndef gaussian_peak(x, centre, width, height):\n    \"\"\"\n    Generate a Gaussian absorption peak.\n    \n    Parameters:\n        x      : array of wavenumber values\n        centre : peak centre position (cm-1)\n        width  : peak width at half-maximum (cm-1), controls broadness\n        height : peak height (absorbance units)\n    \n    Returns:\n        Array of absorbance values for this peak\n    \"\"\"\n    return height * np.exp(-0.5 * ((x - centre) / width) ** 2)\n\n\ndef generate_spectrum(class_type, sample_idx):\n    \"\"\"\n    Generate a simulated FTIR spectrum for a given oil class.\n    \n    Each sample has realistic individual variation controlled by\n    a deterministic per-sample RNG (seeded from sample_idx), so\n    the same sample_idx always produces the same spectrum.\n    \n    Sources of variation:\n        - Peak position jitter (+/- ~3 cm-1): instrument/temperature drift\n        - Intensity variation (+/- ~15%): natural compositional differences\n        - Sample-specific baseline: pathlength and sample preparation\n        - Three planted outlier samples for teaching outlier detection\n    \n    Parameters:\n        class_type : str \u2014 \"EVOO\", \"ROO\", or \"Adulterated\"\n        sample_idx : int \u2014 unique sample index (0-based), drives per-sample variation\n    \n    Returns:\n        1D numpy array of simulated absorbance values\n    \"\"\"\n    spectrum = np.zeros_like(wavenumbers)\n    \n    # --- Per-sample random number generator ---\n    # Each sample gets its own RNG seeded deterministically from sample_idx,\n    # ensuring reproducibility while giving each sample unique variation.\n    # The multiplier 7 spaces seeds apart to reduce correlation.\n    rng = np.random.RandomState(11088 + sample_idx * 7)\n    \n    # --- Per-sample variation parameters ---\n    # Peak position jitter: sigma = 1.5 cm-1, so 95% of shifts are within +/- 3 cm-1\n    # This simulates small instrument calibration differences between measurements\n    pos_jitter = rng.normal(0, 1.5, 10)  # 10 values, one per peak\n    \n    # Intensity variation: sigma = 7%, so 95% of samples are within +/- 14%\n    # This simulates natural compositional variation within each class\n    # (e.g., not all EVOOs have identical oleic acid content)\n    int_variation = 1.0 + rng.normal(0, 0.07, 10)\n    \n    # Sample-specific baseline: slope and offset\n    # Simulates different ATR crystal contact, pathlength, or sample thickness\n    baseline_slope = rng.normal(0, 0.003)\n    baseline_offset = rng.normal(0, 0.015)\n    \n    # --- Common peaks present in ALL olive oil classes ---\n    # These are the fundamental vibrational modes of triglycerides\n    spectrum += gaussian_peak(wavenumbers, 2925 + pos_jitter[0], 30, 0.80 * int_variation[0])  # C-H asymmetric stretch (CH2)\n    spectrum += gaussian_peak(wavenumbers, 2854 + pos_jitter[1], 25, 0.60 * int_variation[1])  # C-H symmetric stretch (CH2)\n    spectrum += gaussian_peak(wavenumbers, 1745 + pos_jitter[2], 20, 0.90 * int_variation[2])  # C=O ester stretch (triglyceride backbone)\n    spectrum += gaussian_peak(wavenumbers, 1465 + pos_jitter[3], 15, 0.35 * int_variation[3])  # C-H bending (scissoring)\n    spectrum += gaussian_peak(wavenumbers, 1160 + pos_jitter[4], 25, 0.50 * int_variation[4])  # C-O stretch (ester linkage)\n    spectrum += gaussian_peak(wavenumbers, 720  + pos_jitter[5], 12, 0.25 * int_variation[5])  # C-H rocking (long-chain alkyl)\n    \n    # --- Class-specific spectral differences ---\n    # These reflect genuine chemical differences between oil types\n    if class_type == \"EVOO\":\n        # EVOO has high unsaturation (oleic, linoleic acid) and polyphenols\n        spectrum += gaussian_peak(wavenumbers, 3005 + pos_jitter[6], 15, 0.30 * int_variation[6])  # Strong =C-H stretch (cis double bonds)\n        spectrum += gaussian_peak(wavenumbers, 1650 + pos_jitter[7], 12, 0.08 * int_variation[7])  # Minor peak from polyphenol/water\n    \n    elif class_type == \"ROO\":\n        # Refined oil: reduced unsaturation markers, altered ester profile\n        spectrum += gaussian_peak(wavenumbers, 3005 + pos_jitter[6], 15, 0.18 * int_variation[6])  # Weaker =C-H (some unsaturation lost in refining)\n        spectrum += gaussian_peak(wavenumbers, 1745 + pos_jitter[2], 20, 0.05 * int_variation[7])  # Slightly modified ester peak shape\n    \n    elif class_type == \"Adulterated\":\n        # Adulterated with cheaper oils (sunflower, hazelnut): different fatty acid profile\n        spectrum += gaussian_peak(wavenumbers, 3005 + pos_jitter[6], 15, 0.12 * int_variation[6])  # Much weaker =C-H (lower unsaturation type)\n        spectrum += gaussian_peak(wavenumbers, 2925 + pos_jitter[0], 30, 0.10 * int_variation[7])  # Extra C-H intensity (sunflower oil contribution)\n        spectrum += gaussian_peak(wavenumbers, 1160 + pos_jitter[4], 25, 0.08 * int_variation[8])  # Altered C-O profile (different triglycerides)\n    \n    # --- Outlier samples ---\n    # Three samples have intentional anomalies to teach outlier detection.\n    # Students should be able to identify these using Hotelling's T2.\n    if sample_idx == 3:\n        # EVOO_04: borderline quality \u2014 unusually low unsaturation with refined character.\n        # This sample should appear between the EVOO and ROO clusters in PCA.\n        spectrum += gaussian_peak(wavenumbers, 3005, 15, -0.12)  # Reduce the =C-H peak\n        spectrum += gaussian_peak(wavenumbers, 1745, 20, 0.08)   # Stronger ester (more like ROO)\n    \n    elif sample_idx == 22:\n        # ROO_11: ATR crystal artefact \u2014 a broad spurious bump around 2100 cm-1.\n        # This is a common measurement error in ATR-FTIR and should be obvious in spectra.\n        spectrum += gaussian_peak(wavenumbers, 2100, 80, 0.15)\n    \n    elif sample_idx == 31:\n        # Adulterated_05: very subtle adulteration \u2014 hard to detect.\n        # The =C-H peak is partially restored, making this sample overlap with ROO.\n        spectrum += gaussian_peak(wavenumbers, 3005, 15, 0.10)\n    \n    # --- Measurement noise ---\n    # White Gaussian noise with sigma = 0.012 absorbance units,\n    # typical for a well-maintained benchtop FTIR instrument\n    noise = rng.normal(0, 0.012, len(wavenumbers))\n    \n    # --- Baseline drift ---\n    # Combination of a sinusoidal component (optical interference fringes)\n    # and a linear component (sample-specific pathlength/preparation)\n    baseline_drift = (0.02 * np.sin(wavenumbers / 800)\n                      + baseline_slope * (wavenumbers - 2200) / 1800\n                      + baseline_offset)\n    \n    spectrum += noise + baseline_drift\n    return spectrum\n\n\n# --- Generate the full dataset ---\n# 15 EVOO + 12 ROO + 10 Adulterated = 37 samples total\n# This is a realistic sample size for a teaching practical\nclasses = [\"EVOO\"] * 15 + [\"ROO\"] * 12 + [\"Adulterated\"] * 10\nsample_ids = [f\"{c}_{i+1:02d}\" for i, c in enumerate(classes)]\n\n# Generate all spectra as a 2D array (samples x wavenumbers)\nspectra = np.array([generate_spectrum(c, i) for i, c in enumerate(classes)])\n\n# --- Create pandas DataFrames ---\n# Spectral data: columns are wavenumber values as strings (standard for spectral data)\nspectral_columns = [f\"{w:.1f}\" for w in wavenumbers]\ndf_spectra = pd.DataFrame(spectra, columns=spectral_columns)\ndf_spectra.insert(0, \"Sample_ID\", sample_ids)  # First column is sample identifier\n\n# Metadata: maps each sample to its class label\ndf_metadata = pd.DataFrame({\"Sample_ID\": sample_ids, \"Class\": classes})\n\n# --- Report ---\nprint(f\"Generated {len(df_spectra)} spectra with {len(wavenumbers)} wavenumber points.\")\nprint(f\"Classes: {dict(zip(*np.unique(classes, return_counts=True)))}\")\nprint(f\"\\nSpectral data shape: {df_spectra.shape}\")\nprint(f\"Wavenumber range: {wavenumbers[-1]:.0f} \u2013 {wavenumbers[0]:.0f} cm\u207b\u00b9\")\nprint(f\"\\nNote: Samples EVOO_04, ROO_11, and Adulterated_05 have intentional\")\nprint(f\"anomalies for outlier detection exercises.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Upload (Your Own Data)\n\nUse this section to upload your own spectral data. You need **two CSV files**:\n\n1. **Spectral data** \u2014 rows = samples, columns = spectral variables (wavenumbers or chemical shifts). First column should be `Sample_ID`.\n2. **Metadata** \u2014 at minimum two columns: `Sample_ID` and `Class`.\n\n> **If using the demo dataset**, skip this cell \u2014 the data is already loaded above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# UPLOAD YOUR OWN DATA (skip if using demo dataset)\n# ============================================================\n# Uncomment and run this cell to upload your own files.\n# The Colab files.upload() widget will prompt you to select files\n# from your local computer.\n\n# from google.colab import files\n#\n# # --- Upload spectral data ---\n# print(\"Upload your SPECTRAL DATA CSV:\")\n# uploaded = files.upload()                            # Opens file picker\n# spectral_filename = list(uploaded.keys())[0]         # Get the uploaded filename\n# df_spectra = pd.read_csv(spectral_filename)          # Parse CSV into DataFrame\n#\n# # --- Upload metadata ---\n# print(\"\\nUpload your METADATA CSV:\")\n# uploaded = files.upload()\n# metadata_filename = list(uploaded.keys())[0]\n# df_metadata = pd.read_csv(metadata_filename)\n#\n# # --- Verify upload ---\n# print(f\"\\nSpectral data: {df_spectra.shape[0]} samples, {df_spectra.shape[1]-1} variables\")\n# print(f\"Metadata: {df_metadata.shape[0]} samples, columns: {list(df_metadata.columns)}\")\n# print(f\"Classes found: {df_metadata['Class'].value_counts().to_dict()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration\n\nBefore any analysis, it is essential to **visually inspect your raw spectra**. Look for:\n- Obvious outliers or failed measurements\n- Baseline drift (common in FTIR)\n- Differences in overall intensity between samples\n- Key absorption bands that differ between groups"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PREPARE DATA MATRICES\n# ============================================================\n# Convert the DataFrames into numpy arrays suitable for analysis.\n# X_raw will be the spectral matrix used throughout the notebook.\n\n# Extract the numeric spectral data (drop the Sample_ID column)\nX_raw = df_spectra.drop(columns=[\"Sample_ID\"]).values.astype(float)\n\n# Keep sample IDs and variable names as separate arrays for labelling plots\nsample_ids = df_spectra[\"Sample_ID\"].values\nvariable_names = df_spectra.drop(columns=[\"Sample_ID\"]).columns.values\n\n# --- Determine x-axis type ---\n# If variable names can be converted to numbers, they represent wavenumbers (FTIR)\n# or chemical shifts (NMR). Otherwise, use a generic variable index.\ntry:\n    x_axis = np.array([float(v) for v in variable_names])\n    # FTIR convention: wavenumbers decrease left-to-right (4000 -> 400)\n    # NMR convention: chemical shifts increase left-to-right\n    x_label = \"Wavenumber (cm\u207b\u00b9)\" if x_axis[0] > x_axis[-1] else \"Chemical shift (ppm)\"\nexcept ValueError:\n    x_axis = np.arange(X_raw.shape[1])\n    x_label = \"Variable index\"\n\n# --- Merge class labels ---\n# Join metadata to spectral data on Sample_ID to ensure correct alignment\nmerged = df_spectra[[\"Sample_ID\"]].merge(df_metadata, on=\"Sample_ID\", how=\"left\")\nclass_labels = merged[\"Class\"].values\nunique_classes = np.unique(class_labels)\n\n# --- Summary ---\nprint(f\"Spectral matrix X: {X_raw.shape[0]} samples \u00d7 {X_raw.shape[1]} variables\")\nprint(f\"Classes: {list(unique_classes)}\")\nprint(f\"X-axis: {x_label}, range {x_axis.min():.1f} \u2013 {x_axis.max():.1f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# INTERACTIVE RAW SPECTRA PLOT\n# ============================================================\n# Plot all raw spectra colour-coded by class using Plotly.\n# Hover over any line to see the sample ID \u2014 useful for spotting outliers.\n\nfig = go.Figure()\n\n# --- Colour palette ---\n# Use Plotly's Set2 qualitative palette (colour-blind friendly)\ncolours = px.colors.qualitative.Set2\n\n# Also create hex versions for matplotlib compatibility (used later in HCA)\ndef plotly_rgb_to_hex(rgb_str):\n    \"\"\"Convert Plotly 'rgb(r,g,b)' string to matplotlib-compatible '#rrggbb'.\"\"\"\n    vals = rgb_str.replace(\"rgb(\", \"\").replace(\")\", \"\").split(\",\")\n    return \"#{:02x}{:02x}{:02x}\".format(int(vals[0]), int(vals[1]), int(vals[2]))\n\nhex_colours = [plotly_rgb_to_hex(c) if c.startswith(\"rgb\") else c for c in colours]\n\n# --- Plot each class as a group ---\nfor i, cls in enumerate(unique_classes):\n    mask = class_labels == cls  # Boolean mask for samples of this class\n    for j, idx in enumerate(np.where(mask)[0]):\n        fig.add_trace(go.Scatter(\n            x=x_axis,\n            y=X_raw[idx],\n            name=cls if j == 0 else None,          # Only show class name once in legend\n            legendgroup=cls,                        # Group all traces of same class\n            showlegend=(j == 0),\n            line=dict(color=colours[i % len(colours)], width=0.8),\n            hovertext=sample_ids[idx],              # Show sample ID on hover\n            hoverinfo=\"text+y\",\n            opacity=0.7\n        ))\n\n# Reverse x-axis for FTIR (conventional display: high -> low wavenumber)\nif x_axis[0] > x_axis[-1]:\n    fig.update_xaxes(autorange=\"reversed\")\n\nfig.update_layout(\n    title=\"Raw Spectra by Class\",\n    xaxis_title=x_label,\n    yaxis_title=\"Absorbance / Intensity\",\n    template=\"plotly_white\",\n    height=500,\n    hovermode=\"closest\"\n)\nfig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# SUMMARY STATISTICS & MISSING VALUES\n# ============================================================\n# Basic data quality checks before analysis. Look for:\n# - Missing/infinite values (would break PCA)\n# - Unusual intensity ranges (suggests scaling issues)\n# - Large differences between class means (good sign for discrimination)\n\nprint(\"=== Data Quality Check ===\")\nprint(f\"Missing values: {np.isnan(X_raw).sum()}\")\nprint(f\"Infinite values: {np.isinf(X_raw).sum()}\")\nprint(f\"\\nIntensity range: {X_raw.min():.4f} to {X_raw.max():.4f}\")\nprint(f\"Mean intensity:  {X_raw.mean():.4f} \u00b1 {X_raw.std():.4f}\")\n\n# Per-class summary \u2014 large differences here suggest the classes are spectrally distinct\nprint(\"\\n=== Per-Class Mean Intensity ===\")\nfor cls in unique_classes:\n    mask = class_labels == cls\n    print(f\"  {cls}: {X_raw[mask].mean():.4f} \u00b1 {X_raw[mask].std():.4f} (n={mask.sum()})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spectral Preprocessing\n\nRaw spectra typically contain artefacts that must be removed before multivariate analysis:\n\n- **Baseline drift** \u2014 gradual shift in the spectrum baseline due to scattering or instrument effects. Corrected using asymmetric least squares (ALS) smoothing or polynomial fitting.\n- **Intensity variation** \u2014 differences in overall signal intensity between samples due to pathlength or concentration differences. Corrected by normalisation (SNV, min-max, or area normalisation).\n- **Noise and band overlap** \u2014 high-frequency noise or overlapping peaks can be resolved using Savitzky-Golay derivatives, which also remove constant and linear baseline offsets.\n\n> **Key principle:** Always visualise the effect of each preprocessing step. Over-processing can destroy genuine chemical information."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PREPROCESSING FUNCTIONS\n# ============================================================\n# These functions implement standard spectral preprocessing methods.\n# Each can be applied independently or in combination.\n\ndef baseline_als(y, lam=1e6, p=0.01, niter=10):\n    \"\"\"\n    Asymmetric Least Squares baseline correction (Eilers & Boelens, 2005).\n    \n    Fits a smooth curve that follows the lower envelope of the spectrum.\n    The baseline is then subtracted to remove drift.\n    \n    Parameters:\n        y     : 1D array \u2014 single spectrum\n        lam   : float \u2014 smoothness parameter. Larger values give a smoother\n                baseline. Typical range: 1e4 (flexible) to 1e8 (very smooth).\n        p     : float \u2014 asymmetry parameter. Smaller values force the baseline\n                closer to the lower envelope. Typical range: 0.001 to 0.05.\n        niter : int \u2014 number of reweighting iterations (10 is usually sufficient)\n    \n    Returns:\n        1D array \u2014 the estimated baseline (subtract from y to correct)\n    \"\"\"\n    L = len(y)\n    # Second-difference matrix penalises curvature in the baseline\n    D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L - 2))\n    w = np.ones(L)  # Initial weights (uniform)\n    for _ in range(niter):\n        W = sparse.spdiags(w, 0, L, L)       # Diagonal weight matrix\n        Z = W + lam * D.dot(D.transpose())    # Penalised least squares system\n        z = sparse.linalg.spsolve(Z, w * y)   # Solve for baseline\n        # Asymmetric reweighting: points above baseline get weight p,\n        # points below get weight (1-p). Since p << 1, the baseline\n        # is pulled toward the lower envelope.\n        w = p * (y > z) + (1 - p) * (y < z)\n    return z\n\n\ndef snv(X):\n    \"\"\"\n    Standard Normal Variate (SNV) normalisation.\n    \n    Each spectrum is independently centred (subtract its mean) and\n    scaled (divide by its standard deviation). This removes multiplicative\n    and additive scatter effects that vary between samples.\n    \n    Parameters:\n        X : 2D array (samples x variables) \u2014 spectral matrix\n    \n    Returns:\n        2D array \u2014 SNV-normalised spectra\n    \"\"\"\n    X_snv = np.zeros_like(X)\n    for i in range(X.shape[0]):\n        X_snv[i] = (X[i] - np.mean(X[i])) / np.std(X[i])\n    return X_snv\n\n\ndef area_normalise(X):\n    \"\"\"\n    Area normalisation: scale each spectrum so its total absolute area = 1.\n    \n    Commonly used for NMR data where total signal intensity should be\n    proportional to concentration. Not ideal for FTIR where negative\n    absorbance values can occur after baseline correction.\n    \n    Parameters:\n        X : 2D array (samples x variables)\n    \n    Returns:\n        2D array \u2014 area-normalised spectra\n    \"\"\"\n    return X / np.abs(X).sum(axis=1, keepdims=True)\n\n\ndef minmax_normalise(X):\n    \"\"\"\n    Min-max normalisation: scale each spectrum to the [0, 1] range.\n    \n    Simple but can amplify noise in low-intensity spectra.\n    \n    Parameters:\n        X : 2D array (samples x variables)\n    \n    Returns:\n        2D array \u2014 min-max normalised spectra\n    \"\"\"\n    mins = X.min(axis=1, keepdims=True)\n    maxs = X.max(axis=1, keepdims=True)\n    return (X - mins) / (maxs - mins)\n\n\ndef savgol_derivative(X, window_length=15, polyorder=2, deriv=1):\n    \"\"\"\n    Savitzky-Golay derivative filter.\n    \n    Smooths the spectrum with a local polynomial fit, then computes the\n    derivative analytically. Benefits:\n    - 1st derivative removes constant baseline offsets\n    - 2nd derivative removes linear baseline slopes\n    - Both enhance resolution of overlapping peaks\n    \n    Parameters:\n        X             : 2D array (samples x variables)\n        window_length : int (must be odd) \u2014 larger = more smoothing\n        polyorder     : int \u2014 polynomial degree (2 or 3 typical)\n        deriv         : int \u2014 derivative order (1 = first, 2 = second)\n    \n    Returns:\n        2D array \u2014 derivative spectra\n    \"\"\"\n    return signal.savgol_filter(X, window_length, polyorder, deriv=deriv, axis=1)\n\n\nprint(\"\u2705 Preprocessing functions defined.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Apply Preprocessing\n\nModify the settings below to experiment with different preprocessing combinations. A typical workflow for FTIR data is: **baseline correction \u2192 SNV \u2192 (optional) 1st derivative**.\n\nFor NMR data, you might skip baseline correction and use: **area normalisation \u2192 (optional) binning**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PREPROCESSING SETTINGS \u2014 MODIFY THESE\n# ============================================================\n# Change these parameters and re-run this cell to see the effect.\n# The preprocessed data (X_processed) is used by all subsequent analyses.\n\n# Step 1: Baseline correction (ALS)\nAPPLY_BASELINE = True           # Set to False to skip\nBASELINE_LAMBDA = 1e6           # Smoothness: try 1e4 (flexible) to 1e8 (rigid)\nBASELINE_P = 0.01               # Asymmetry: try 0.001 (tight fit) to 0.05 (loose)\n\n# Step 2: Normalisation\nNORMALISATION = \"snv\"            # Options: \"snv\", \"area\", \"minmax\", \"none\"\n\n# Step 3: Savitzky-Golay derivative\nAPPLY_DERIVATIVE = False         # Set to True to apply\nSG_WINDOW = 15                   # Window length (must be odd, larger = smoother)\nSG_POLYORDER = 2                 # Polynomial order (2 or 3)\nSG_DERIV = 1                     # Derivative order: 1 (removes offsets) or 2 (removes slopes)\n\n# ============================================================\n# APPLY PREPROCESSING PIPELINE\n# ============================================================\n# Each step transforms X_processed in place. The order matters:\n# baseline correction first (removes drift), then normalisation\n# (removes intensity variation), then optionally derivatives.\n\nX_processed = X_raw.copy()  # Start from raw data each time\n\n# Step 1: Baseline correction\nif APPLY_BASELINE:\n    print(\"Applying baseline correction (ALS)...\")\n    for i in range(X_processed.shape[0]):\n        bl = baseline_als(X_processed[i], lam=BASELINE_LAMBDA, p=BASELINE_P)\n        X_processed[i] = X_processed[i] - bl  # Subtract estimated baseline\n\n# Step 2: Normalisation\nif NORMALISATION == \"snv\":\n    print(\"Applying SNV normalisation...\")\n    X_processed = snv(X_processed)\nelif NORMALISATION == \"area\":\n    print(\"Applying area normalisation...\")\n    X_processed = area_normalise(X_processed)\nelif NORMALISATION == \"minmax\":\n    print(\"Applying min-max normalisation...\")\n    X_processed = minmax_normalise(X_processed)\nelse:\n    print(\"No normalisation applied.\")\n\n# Step 3: Derivative\nif APPLY_DERIVATIVE:\n    deriv_suffix = \"st\" if SG_DERIV == 1 else \"nd\"\n    print(f\"Applying Savitzky-Golay {SG_DERIV}{deriv_suffix} derivative...\")\n    X_processed = savgol_derivative(X_processed, SG_WINDOW, SG_POLYORDER, SG_DERIV)\n\nprint(f\"\\n\u2705 Preprocessing complete. Matrix shape: {X_processed.shape}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VISUALISE PREPROCESSING EFFECT\n# ============================================================\n# Side-by-side comparison of raw vs preprocessed spectra.\n# This is essential for quality control \u2014 check that:\n# - Baseline drift has been removed (spectra should be flat outside peaks)\n# - Normalisation has equalised overall intensity\n# - No genuine peaks have been destroyed\n\nfig = make_subplots(\n    rows=2, cols=1,\n    subplot_titles=(\"Before preprocessing\", \"After preprocessing\"),\n    shared_xaxes=True,        # Same x-axis for easy comparison\n    vertical_spacing=0.08\n)\n\nfor i, cls in enumerate(unique_classes):\n    mask = class_labels == cls\n    for j, idx in enumerate(np.where(mask)[0]):\n        # Top panel: raw spectra\n        fig.add_trace(go.Scatter(\n            x=x_axis, y=X_raw[idx], name=cls if j == 0 else None,\n            legendgroup=cls, showlegend=(j == 0),\n            line=dict(color=colours[i % len(colours)], width=0.6), opacity=0.6\n        ), row=1, col=1)\n        # Bottom panel: preprocessed spectra\n        fig.add_trace(go.Scatter(\n            x=x_axis, y=X_processed[idx], name=cls if j == 0 else None,\n            legendgroup=cls, showlegend=False,\n            line=dict(color=colours[i % len(colours)], width=0.6), opacity=0.6\n        ), row=2, col=1)\n\n# Reverse x-axis for FTIR convention\nif x_axis[0] > x_axis[-1]:\n    fig.update_xaxes(autorange=\"reversed\")\n\nfig.update_layout(title=\"Preprocessing Comparison\", template=\"plotly_white\", height=700)\nfig.update_xaxes(title_text=x_label, row=2, col=1)\nfig.update_yaxes(title_text=\"Absorbance\", row=1, col=1)\nfig.update_yaxes(title_text=\"Preprocessed\", row=2, col=1)\nfig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Principal Component Analysis (PCA)\n\nPCA is an **unsupervised** method that reduces the dimensionality of spectral data by finding directions (principal components) of maximum variance. It is typically the first step in chemometric analysis because it:\n\n- Reveals natural groupings (or lack thereof) in the data\n- Identifies outliers\n- Shows which spectral regions contribute most to variation (via loadings)\n- Helps determine how many latent variables are needed for supervised models\n\n> **Important:** PCA does not use class labels \u2014 any separation seen in the scores plot reflects genuine chemical differences captured by the spectral data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PCA \u2014 FIT AND EXPLORE\n# ============================================================\nN_COMPONENTS = 10  # Number of PCs to compute (more than needed, we'll inspect the scree plot)\n\n# --- Mean-centring ---\n# Standard practice for PCA on spectral data: subtract the mean spectrum.\n# We do NOT scale by standard deviation (with_std=False) because all\n# variables share the same unit (absorbance) and scaling would distort\n# the spectral shape.\nscaler = StandardScaler(with_std=False)\nX_mc = scaler.fit_transform(X_processed)\n\n# --- Fit PCA ---\n# n_components is limited to min(n_samples, n_variables) by definition\npca = PCA(n_components=min(N_COMPONENTS, X_mc.shape[0], X_mc.shape[1]))\nscores = pca.fit_transform(X_mc)       # Project data into PC space (n_samples x n_PCs)\nloadings = pca.components_              # PC loadings (n_PCs x n_variables)\nexplained_var = pca.explained_variance_ratio_ * 100  # Percentage variance per PC\n\n# --- Report explained variance ---\nprint(f\"PCA fitted with {pca.n_components_} components.\")\nprint(f\"\\nExplained variance per PC:\")\nfor i, ev in enumerate(explained_var):\n    cumulative = explained_var[:i+1].sum()\n    bar = \"\u2588\" * int(ev)  # Visual bar proportional to variance\n    print(f\"  PC{i+1:2d}: {ev:5.1f}% {bar}  (cumulative: {cumulative:.1f}%)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# SCREE PLOT\n# ============================================================\n# The scree plot shows how much variance each PC captures.\n# Use it to decide how many PCs are \"meaningful\":\n# - Look for an \"elbow\" where the curve flattens\n# - Typically, PCs beyond the elbow capture mostly noise\n\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Bar chart: individual variance per PC\nfig.add_trace(go.Bar(\n    x=[f\"PC{i+1}\" for i in range(len(explained_var))],\n    y=explained_var,\n    name=\"Individual\",\n    marker_color=\"steelblue\"\n), secondary_y=False)\n\n# Line chart: cumulative variance\nfig.add_trace(go.Scatter(\n    x=[f\"PC{i+1}\" for i in range(len(explained_var))],\n    y=np.cumsum(explained_var),\n    name=\"Cumulative\",\n    mode=\"lines+markers\",\n    marker=dict(color=\"firebrick\"),\n    line=dict(color=\"firebrick\")\n), secondary_y=True)\n\nfig.update_layout(title=\"Scree Plot\", template=\"plotly_white\", height=400)\nfig.update_yaxes(title_text=\"Explained variance (%)\", secondary_y=False)\nfig.update_yaxes(title_text=\"Cumulative (%)\", secondary_y=True, range=[0, 105])\nfig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PCA SCORES PLOT (interactive with 95% confidence ellipses)\n# ============================================================\n# The scores plot shows each sample as a point in PC space.\n# Samples that are spectrally similar cluster together.\n# Change PC_X and PC_Y to explore different PC combinations.\n\nPC_X = 1  # Which PC on x-axis (1-indexed, so PC1 = first component)\nPC_Y = 2  # Which PC on y-axis\n\n# Build a DataFrame for Plotly Express (needs tidy data format)\nscores_df = pd.DataFrame({\n    \"Sample_ID\": sample_ids,\n    \"Class\": class_labels,\n    f\"PC{PC_X}\": scores[:, PC_X - 1],\n    f\"PC{PC_Y}\": scores[:, PC_Y - 1]\n})\n\nfig = px.scatter(\n    scores_df, x=f\"PC{PC_X}\", y=f\"PC{PC_Y}\",\n    color=\"Class\", hover_name=\"Sample_ID\",\n    title=f\"PCA Scores Plot: PC{PC_X} vs PC{PC_Y}\",\n    labels={\n        f\"PC{PC_X}\": f\"PC{PC_X} ({explained_var[PC_X-1]:.1f}%)\",\n        f\"PC{PC_Y}\": f\"PC{PC_Y} ({explained_var[PC_Y-1]:.1f}%)\"\n    },\n    color_discrete_sequence=px.colors.qualitative.Set2\n)\n\n# --- Add 95% confidence ellipses ---\n# These show the region where 95% of samples from each class are expected\n# to fall, assuming a bivariate normal distribution. Calculated from the\n# eigenvalues/vectors of the 2x2 covariance matrix and the chi-squared\n# critical value for 2 degrees of freedom (5.991 at 95%).\nfor cls in unique_classes:\n    mask = class_labels == cls\n    pc_x = scores[mask, PC_X - 1]\n    pc_y = scores[mask, PC_Y - 1]\n    \n    if len(pc_x) < 3:  # Need at least 3 points for a covariance matrix\n        continue\n    \n    # Covariance matrix of the two PCs for this class\n    cov = np.cov(pc_x, pc_y)\n    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n    angle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n    chi2_val = 5.991  # 95% confidence, 2 degrees of freedom\n    \n    # Generate ellipse boundary points\n    theta = np.linspace(0, 2 * np.pi, 100)\n    a = np.sqrt(eigenvalues[1] * chi2_val)  # Semi-major axis\n    b = np.sqrt(eigenvalues[0] * chi2_val)  # Semi-minor axis\n    ellipse_x = a * np.cos(theta)\n    ellipse_y = b * np.sin(theta)\n    \n    # Rotate ellipse to align with eigenvectors, then translate to class centre\n    cos_a, sin_a = np.cos(np.radians(angle)), np.sin(np.radians(angle))\n    rot_x = cos_a * ellipse_x - sin_a * ellipse_y + pc_x.mean()\n    rot_y = sin_a * ellipse_x + cos_a * ellipse_y + pc_y.mean()\n    \n    fig.add_trace(go.Scatter(\n        x=rot_x, y=rot_y, mode=\"lines\",\n        line=dict(dash=\"dash\", width=1),\n        showlegend=False, hoverinfo=\"skip\"\n    ))\n\nfig.update_layout(template=\"plotly_white\", height=550, width=700)\nfig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PCA LOADINGS PLOT\n# ============================================================\n# Loadings show which spectral variables (wavenumbers) contribute\n# most to each PC. Peaks in the loadings correspond to absorption\n# bands that drive the separation seen in the scores plot.\n#\n# Positive loadings: variables that increase the PC score\n# Negative loadings: variables that decrease the PC score\n\nLOADING_PC = 1  # Which PC loadings to plot (1-indexed)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=x_axis, y=loadings[LOADING_PC - 1],\n    mode=\"lines\", name=f\"PC{LOADING_PC} loadings\",\n    line=dict(color=\"steelblue\")\n))\n\n# Zero line for reference\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"grey\")\n\nif x_axis[0] > x_axis[-1]:\n    fig.update_xaxes(autorange=\"reversed\")\n\nfig.update_layout(\n    title=f\"PC{LOADING_PC} Loadings ({explained_var[LOADING_PC-1]:.1f}% variance)\",\n    xaxis_title=x_label,\n    yaxis_title=\"Loading\",\n    template=\"plotly_white\",\n    height=400\n)\nfig.show()\n\n# --- Report top contributing variables ---\n# Sort by absolute loading value to find the most influential wavenumbers\nprint(f\"\\nTop contributing variables for PC{LOADING_PC}:\")\nabs_loadings = np.abs(loadings[LOADING_PC - 1])\ntop_idx = np.argsort(abs_loadings)[::-1][:10]\nfor idx in top_idx:\n    print(f\"  {variable_names[idx]:>10s}: {loadings[LOADING_PC-1][idx]:+.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Hotelling's T\u00b2 \u2014 Outlier Detection\n\nHotelling's T\u00b2 statistic identifies samples that are unusually far from the centre of the PCA model. Samples exceeding the 95% or 99% confidence limit may be outliers that distort subsequent analysis.\n\n> **Action:** If outliers are identified, investigate them. They may represent measurement errors, contamination, or genuinely unusual samples. Remove them only with justification."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# HOTELLING'S T\u00b2 OUTLIER DETECTION\n# ============================================================\n# Hotelling's T\u00b2 is the multivariate equivalent of a z-score.\n# It measures how far each sample is from the PCA model centre,\n# accounting for the correlation structure between PCs.\n#\n# T\u00b2 = score_vector @ inverse_covariance_matrix @ score_vector.T\n#\n# Critical values come from the F-distribution, scaled for sample size.\n\nN_PC_HOTELLING = 3  # Number of PCs to include (usually 2-5)\n\n# Extract scores for the selected PCs\nscores_subset = scores[:, :N_PC_HOTELLING]\n\n# Inverse covariance matrix of the scores\ncov_inv = np.linalg.inv(np.cov(scores_subset.T))\n\n# Compute T\u00b2 for each sample\nT2 = np.array([s @ cov_inv @ s.T for s in scores_subset])\n\n# --- Critical values from F-distribution ---\n# Formula: T\u00b2_crit = p(n-1)(n+1) / n(n-p) * F_crit(p, n-p)\n# where p = number of PCs, n = number of samples\nn = X_mc.shape[0]\np = N_PC_HOTELLING\nT2_95 = (p * (n - 1) * (n + 1)) / (n * (n - p)) * f_dist.ppf(0.95, p, n - p)\nT2_99 = (p * (n - 1) * (n + 1)) / (n * (n - p)) * f_dist.ppf(0.99, p, n - p)\n\n# --- Plot ---\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=list(range(n)), y=T2,\n    mode=\"markers+text\",\n    text=sample_ids,\n    textposition=\"top center\",\n    textfont=dict(size=8),\n    marker=dict(\n        # Colour each point by its class (using hex colours for compatibility)\n        color=[hex_colours[list(unique_classes).index(c) % len(hex_colours)] for c in class_labels],\n        size=8\n    ),\n    hovertext=sample_ids,\n    name=\"Samples\"\n))\n\n# Confidence limit lines\nfig.add_hline(y=T2_95, line_dash=\"dash\", line_color=\"orange\",\n              annotation_text=\"95% limit\", annotation_position=\"top right\")\nfig.add_hline(y=T2_99, line_dash=\"dash\", line_color=\"red\",\n              annotation_text=\"99% limit\", annotation_position=\"top right\")\n\nfig.update_layout(\n    title=f\"Hotelling's T\u00b2 (based on {N_PC_HOTELLING} PCs)\",\n    xaxis_title=\"Sample index\",\n    yaxis_title=\"T\u00b2\",\n    template=\"plotly_white\",\n    height=400\n)\nfig.show()\n\n# --- Report outliers ---\noutliers_95 = sample_ids[T2 > T2_95]\noutliers_99 = sample_ids[T2 > T2_99]\nprint(f\"Samples exceeding 95% limit: {list(outliers_95) if len(outliers_95) > 0 else 'None'}\")\nprint(f\"Samples exceeding 99% limit: {list(outliers_99) if len(outliers_99) > 0 else 'None'}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical Cluster Analysis (HCA)\n\nHCA is another **unsupervised** method that groups samples based on their similarity. Unlike PCA, it produces a **dendrogram** \u2014 a tree-like diagram showing how samples cluster together. This is useful for:\n\n- Confirming groupings seen in PCA\n- Identifying sub-groups within a class\n- Assessing how well different classes separate\n\n**Key parameters:**\n- **Distance metric** \u2014 how similarity is measured (Euclidean, correlation, cosine)\n- **Linkage method** \u2014 how clusters are merged (Ward's is generally recommended for spectral data)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# HIERARCHICAL CLUSTER ANALYSIS\n# ============================================================\nDISTANCE_METRIC = \"euclidean\"   # Options: \"euclidean\", \"correlation\", \"cosine\"\nLINKAGE_METHOD = \"ward\"          # Options: \"ward\", \"complete\", \"average\", \"single\"\n# Note: Ward's method minimises within-cluster variance and requires Euclidean distance.\n\n# --- Compute linkage matrix ---\n# Z encodes the dendrogram structure: each row is a merge step\n# [cluster_1, cluster_2, distance, n_samples_in_new_cluster]\nZ = linkage(X_processed, method=LINKAGE_METHOD, metric=DISTANCE_METRIC)\n\n# --- Colour map for class labels ---\n# Uses hex colours (converted from Plotly palette) for matplotlib compatibility\nclass_colour_map = {cls: hex_colours[i % len(hex_colours)] for i, cls in enumerate(unique_classes)}\n\n# --- Plot dendrogram ---\nfig, ax = plt.subplots(figsize=(14, 5))\ndend = dendrogram(\n    Z,\n    labels=sample_ids,       # Label each leaf with sample ID\n    leaf_rotation=90,         # Rotate labels for readability\n    leaf_font_size=8,\n    ax=ax,\n    color_threshold=0         # Colour all links the same (we colour labels instead)\n)\n\n# --- Colour leaf labels by class ---\n# This makes it easy to see whether the dendrogram correctly groups samples by class\nxlbls = ax.get_xmajorticklabels()\nfor lbl in xlbls:\n    sample = lbl.get_text()\n    idx = list(sample_ids).index(sample)\n    lbl.set_color(class_colour_map[class_labels[idx]])\n\nax.set_title(f\"HCA Dendrogram ({LINKAGE_METHOD} linkage, {DISTANCE_METRIC} distance)\")\nax.set_ylabel(\"Distance\")\n\n# Add a legend mapping colours to classes\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=class_colour_map[c], label=c) for c in unique_classes]\nax.legend(handles=legend_elements, loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Partial Least Squares Discriminant Analysis (PLS-DA)\n\nPLS-DA is a **supervised** method that finds latent variables maximising the covariance between the spectral data (X) and the class labels (Y). Unlike PCA, it uses class information to find the most discriminating directions in spectral space.\n\n**Key considerations:**\n- The number of latent variables (LVs) must be optimised by cross-validation to avoid overfitting\n- **VIP scores** (Variable Importance in Projection) identify which spectral regions drive classification\n- Always validate with cross-validation \u2014 never rely on calibration performance alone\n\n> **Overfitting warning:** PLS-DA can easily overfit, especially with many more variables than samples (the typical case for spectral data). Cross-validation is essential."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PLS-DA: ENCODE CLASSES AND SELECT NUMBER OF LVs\n# ============================================================\nMAX_LV = 10  # Maximum number of latent variables to test\n\n# --- Encode class labels ---\n# PLS regression requires numeric Y. For multiclass problems, we use\n# a dummy (one-hot) encoding: each class becomes a column with 0/1 values.\n# Example for 3 classes: EVOO=[1,0,0], ROO=[0,1,0], Adulterated=[0,0,1]\nle = LabelEncoder()\ny_encoded = le.fit_transform(class_labels)  # Integer encoding: 0, 1, 2\n\nn_classes = len(unique_classes)\nY_dummy = np.zeros((len(y_encoded), n_classes))\nfor i, y in enumerate(y_encoded):\n    Y_dummy[i, y] = 1  # Set the column corresponding to this sample's class to 1\n\n# --- Cross-validation to select optimal number of LVs ---\n# We use 5-fold stratified CV: data is split into 5 parts, each class\n# represented proportionally in each fold. The model is trained on 4 folds\n# and tested on the held-out fold, rotated 5 times.\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=11088)\ncv_scores = []\n\nfor n_lv in range(1, MAX_LV + 1):\n    pls = PLSRegression(n_components=n_lv, scale=True)\n    # cross_val_predict returns out-of-fold predictions for every sample\n    y_pred_cv = cross_val_predict(pls, X_processed, Y_dummy, cv=cv)\n    # Convert continuous PLS predictions back to class labels\n    # by selecting the column with the highest predicted value\n    y_pred_class = le.inverse_transform(np.argmax(y_pred_cv, axis=1))\n    acc = accuracy_score(class_labels, y_pred_class)\n    cv_scores.append(acc)\n\n# The optimal number of LVs is the one with highest CV accuracy\noptimal_lv = np.argmax(cv_scores) + 1\n\n# --- Plot CV accuracy vs number of LVs ---\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=list(range(1, MAX_LV + 1)), y=cv_scores,\n    mode=\"lines+markers\", marker=dict(color=\"steelblue\")\n))\nfig.add_vline(x=optimal_lv, line_dash=\"dash\", line_color=\"red\",\n              annotation_text=f\"Optimal: {optimal_lv} LVs\")\n\nfig.update_layout(\n    title=\"PLS-DA: Cross-Validation Accuracy vs Number of LVs\",\n    xaxis_title=\"Number of Latent Variables\",\n    yaxis_title=\"CV Accuracy\",\n    template=\"plotly_white\",\n    height=400,\n    yaxis=dict(range=[0, 1.05])\n)\nfig.show()\n\nprint(f\"\\nOptimal number of LVs: {optimal_lv} (CV accuracy: {cv_scores[optimal_lv-1]:.3f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PLS-DA: FIT FINAL MODEL AND EVALUATE\n# ============================================================\nN_LV = optimal_lv  # Use the optimal value, or override manually here\n\n# Fit PLS-DA on the full dataset with the selected number of LVs\npls_final = PLSRegression(n_components=N_LV, scale=True)\npls_final.fit(X_processed, Y_dummy)\n\n# --- Cross-validated predictions ---\n# These are out-of-fold predictions (each sample predicted by a model\n# that did NOT see it during training), giving an honest accuracy estimate\ny_pred_cv = cross_val_predict(pls_final, X_processed, Y_dummy, cv=cv)\ny_pred_class = le.inverse_transform(np.argmax(y_pred_cv, axis=1))\n\n# --- Confusion matrix ---\n# Rows = true class, columns = predicted class\n# Perfect classification has all counts on the diagonal\ncm = confusion_matrix(class_labels, y_pred_class, labels=le.classes_)\nfig = px.imshow(\n    cm, x=le.classes_, y=le.classes_,\n    labels=dict(x=\"Predicted\", y=\"True\", color=\"Count\"),\n    text_auto=True,                     # Show counts as text in cells\n    color_continuous_scale=\"Blues\",\n    title=f\"PLS-DA Confusion Matrix ({N_LV} LVs, 5-fold CV)\"\n)\nfig.update_layout(height=400, width=500)\nfig.show()\n\n# --- Classification report ---\n# Precision: of all samples predicted as class X, how many truly are X?\n# Recall: of all true class X samples, how many were correctly identified?\n# F1-score: harmonic mean of precision and recall\nprint(\"\\nClassification Report (cross-validated):\")\nprint(classification_report(class_labels, y_pred_class))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PLS-DA SCORES PLOT\n# ============================================================\n# Similar to PCA scores, but the axes (latent variables) are chosen\n# to maximise separation between classes, not just total variance.\n\npls_scores = pls_final.x_scores_  # Calibration scores (n_samples x n_LVs)\n\nLV_X = 1  # LV for x-axis (1-indexed)\nLV_Y = 2  # LV for y-axis\n\nscores_df = pd.DataFrame({\n    \"Sample_ID\": sample_ids,\n    \"Class\": class_labels,\n    f\"LV{LV_X}\": pls_scores[:, LV_X - 1],\n    # If only 1 LV was selected, use zeros for the y-axis\n    f\"LV{LV_Y}\": pls_scores[:, LV_Y - 1] if N_LV >= 2 else np.zeros(len(sample_ids))\n})\n\nfig = px.scatter(\n    scores_df, x=f\"LV{LV_X}\", y=f\"LV{LV_Y}\",\n    color=\"Class\", hover_name=\"Sample_ID\",\n    title=f\"PLS-DA Scores Plot: LV{LV_X} vs LV{LV_Y}\",\n    color_discrete_sequence=px.colors.qualitative.Set2\n)\nfig.update_layout(template=\"plotly_white\", height=550, width=700)\nfig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VIP SCORES (Variable Importance in Projection)\n# ============================================================\n# VIP scores summarise the importance of each spectral variable\n# across all PLS components. They answer: \"which wavenumbers are\n# most important for distinguishing the classes?\"\n#\n# Rule of thumb:\n#   VIP > 1.0 : important variable (contributes above average)\n#   VIP > 0.8 : moderately important\n#   VIP < 0.8 : less important (candidate for removal in variable selection)\n\ndef calculate_vip(pls_model, X, Y):\n    \"\"\"\n    Calculate VIP scores for a fitted PLS model.\n    \n    The VIP for variable j is:\n        VIP_j = sqrt( p * sum_h( SS_h * w_jh^2 ) / sum_h( SS_h ) )\n    \n    where p = number of variables, h = component index,\n    SS_h = sum of squares of Y explained by component h,\n    w_jh = weight of variable j in component h.\n    \n    Parameters:\n        pls_model : fitted PLSRegression object\n        X, Y      : the data matrices used to fit the model\n    \n    Returns:\n        1D array of VIP scores (one per variable)\n    \"\"\"\n    T = pls_model.x_scores_        # X scores (n_samples x n_components)\n    W = pls_model.x_weights_       # X weights (n_variables x n_components)\n    Q = pls_model.y_loadings_      # Y loadings (n_targets x n_components)\n    \n    p_vars = X.shape[1]  # Number of spectral variables\n    h = T.shape[1]       # Number of PLS components\n    \n    # SS_h: variance in Y explained by each component\n    s = np.diag(T.T @ T @ Q.T @ Q)\n    s_total = s.sum()\n    \n    # VIP for each variable\n    vip = np.zeros(p_vars)\n    for i in range(p_vars):\n        # Squared normalised weight for variable i across all components,\n        # weighted by the Y-variance explained by each component\n        weight_sum = np.sum(s * (W[i, :] / np.linalg.norm(W[:, :], axis=0)) ** 2)\n        vip[i] = np.sqrt(p_vars * weight_sum / s_total)\n    \n    return vip\n\n# --- Calculate and plot VIP scores ---\nvip_scores = calculate_vip(pls_final, X_processed, Y_dummy)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=x_axis, y=vip_scores,\n    mode=\"lines\", line=dict(color=\"steelblue\"),\n    hovertext=[f\"{v}: VIP={vip:.2f}\" for v, vip in zip(variable_names, vip_scores)]\n))\n\n# Importance thresholds\nfig.add_hline(y=1.0, line_dash=\"dash\", line_color=\"red\",\n              annotation_text=\"VIP = 1.0 (important)\", annotation_position=\"top right\")\nfig.add_hline(y=0.8, line_dash=\"dot\", line_color=\"orange\",\n              annotation_text=\"VIP = 0.8\", annotation_position=\"top right\")\n\nif x_axis[0] > x_axis[-1]:\n    fig.update_xaxes(autorange=\"reversed\")\n\nfig.update_layout(\n    title=\"VIP Scores \u2014 Spectral Regions Driving Classification\",\n    xaxis_title=x_label,\n    yaxis_title=\"VIP Score\",\n    template=\"plotly_white\",\n    height=400\n)\nfig.show()\n\n# --- Report top VIP variables ---\nprint(f\"\\nTop 10 most important spectral variables:\")\ntop_vip_idx = np.argsort(vip_scores)[::-1][:10]\nfor idx in top_vip_idx:\n    print(f\"  {variable_names[idx]:>10s} cm\u207b\u00b9: VIP = {vip_scores[idx]:.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Orthogonal PLS-DA (OPLS-DA)\n\nOPLS-DA (Trygg & Wold, 2002) is an extension of PLS-DA that separates the variation in X into:\n\n- **Predictive** variation \u2014 correlated with the class labels (Y)\n- **Orthogonal** variation \u2014 structured variation unrelated to classes (e.g., batch effects, instrument drift)\n\nThis separation has two major advantages:\n1. The **predictive scores plot** often shows cleaner class separation than PLS-DA\n2. The **S-plot** (covariance vs correlation) identifies reliable biomarkers for class discrimination\n\n> **Note:** OPLS-DA is most effective for **two-class** comparisons. For multiclass problems, consider running pairwise OPLS-DA models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# OPLS-DA IMPLEMENTATION\n# ============================================================\n# This implements the OPLS algorithm following Trygg & Wold (2002).\n# scikit-learn does not include OPLS-DA, so we build it on top of\n# PLSRegression, using the orthogonal signal correction (OSC) approach.\n\nclass OPLSDA:\n    \"\"\"\n    Orthogonal PLS-DA (Trygg & Wold, 2002).\n    \n    The algorithm works in two stages:\n    1. Iteratively identify and remove orthogonal components from X\n       (variation in X that is uncorrelated with Y)\n    2. Fit a standard PLS model on the filtered X\n    \n    For two-class problems: 1 predictive + n_ortho orthogonal components.\n    \n    Attributes after fitting:\n        t_pred_     : predictive scores (1D array, n_samples)\n        t_ortho_    : orthogonal scores (n_samples x n_ortho)\n        p_pred_     : predictive loadings (1D array, n_variables)\n        R2Y_        : R-squared for Y (goodness of fit)\n        X_filtered_ : X with orthogonal variation removed\n    \"\"\"\n    \n    def __init__(self, n_ortho=1):\n        \"\"\"\n        Parameters:\n            n_ortho : int \u2014 number of orthogonal components to remove.\n                      Usually 1 is sufficient; increase if the predictive\n                      scores plot still shows unwanted structure.\n        \"\"\"\n        self.n_ortho = n_ortho\n    \n    def fit(self, X, y):\n        \"\"\"\n        Fit OPLS-DA model.\n        \n        Parameters:\n            X : 2D array (n_samples x n_variables) \u2014 preprocessed spectra\n            y : 1D array (n_samples,) \u2014 binary class labels (0 or 1)\n        \n        Returns:\n            self (for method chaining)\n        \"\"\"\n        # --- Mean-centre both X and y ---\n        self.X_mean_ = X.mean(axis=0)\n        self.y_mean_ = y.mean()\n        Xc = X - self.X_mean_\n        yc = (y - self.y_mean_).reshape(-1, 1)\n        \n        # Storage for orthogonal components\n        self.t_ortho_ = []\n        self.p_ortho_ = []\n        self.w_ortho_ = []\n        \n        X_filtered = Xc.copy()\n        \n        # --- Iteratively extract orthogonal components ---\n        for i in range(self.n_ortho):\n            # Step 1: Compute PLS weight vector (direction in X most\n            # correlated with y). This is the predictive direction.\n            w = (X_filtered.T @ yc) / (yc.T @ yc)\n            w = w / np.linalg.norm(w)  # Normalise to unit length\n            \n            # Step 2: PLS scores and loadings for this weight\n            t = X_filtered @ w          # Project X onto the weight vector\n            p = (X_filtered.T @ t) / (t.T @ t)  # X loadings\n            \n            # Step 3: Compute orthogonal weight\n            # This is the component of p that is perpendicular to w.\n            # It captures structured variation in X that is NOT related to y.\n            w_ortho = p - (w.T @ p) / (w.T @ w) * w\n            w_ortho = w_ortho / np.linalg.norm(w_ortho)\n            \n            # Step 4: Orthogonal scores and loadings\n            t_ortho = X_filtered @ w_ortho\n            p_ortho = (X_filtered.T @ t_ortho) / (t_ortho.T @ t_ortho)\n            \n            # Step 5: Remove the orthogonal component from X\n            X_filtered = X_filtered - t_ortho @ p_ortho.T\n            \n            # Store for later use (transform, S-plot)\n            self.t_ortho_.append(t_ortho.ravel())\n            self.p_ortho_.append(p_ortho.ravel())\n            self.w_ortho_.append(w_ortho.ravel())\n        \n        # Convert lists to arrays\n        self.t_ortho_ = np.array(self.t_ortho_).T   # (n_samples x n_ortho)\n        self.p_ortho_ = np.array(self.p_ortho_).T   # (n_variables x n_ortho)\n        self.w_ortho_ = np.array(self.w_ortho_).T   # (n_variables x n_ortho)\n        \n        # --- Fit PLS on filtered (orthogonal-corrected) X ---\n        # Now X_filtered contains only predictive + noise variation\n        self.pls_ = PLSRegression(n_components=1, scale=False)\n        self.pls_.fit(X_filtered, yc)\n        \n        # Store predictive scores and loadings\n        self.t_pred_ = self.pls_.x_scores_.ravel()\n        self.p_pred_ = self.pls_.x_loadings_.ravel()\n        self.w_pred_ = self.pls_.x_weights_.ravel()\n        self.X_filtered_ = X_filtered\n        \n        # --- Goodness of fit: R\u00b2Y ---\n        # How much of the variance in y is explained by the predictive component?\n        y_pred = self.pls_.predict(X_filtered)\n        ss_res = np.sum((yc - y_pred) ** 2)   # Residual sum of squares\n        ss_tot = np.sum(yc ** 2)               # Total sum of squares (centred)\n        self.R2Y_ = 1 - ss_res / ss_tot\n        \n        return self\n    \n    def transform(self, X):\n        \"\"\"Apply orthogonal correction to new data (e.g., test set).\"\"\"\n        Xc = X - self.X_mean_\n        for i in range(self.n_ortho):\n            t_ortho = Xc @ self.w_ortho_[:, i:i+1]\n            Xc = Xc - t_ortho @ self.p_ortho_[:, i:i+1].T\n        return Xc\n    \n    def predict(self, X):\n        \"\"\"Predict class for new data.\"\"\"\n        X_filt = self.transform(X)\n        return self.pls_.predict(X_filt).ravel() + self.y_mean_\n    \n    def s_plot_data(self):\n        \"\"\"\n        Generate S-plot data: covariance (p) vs correlation (pcorr).\n        \n        The S-plot visualises reliability vs magnitude for each variable:\n        - p(cov): the loading value (magnitude of contribution)\n        - p(corr): correlation between variable and predictive score (reliability)\n        \n        Variables in the upper-right or lower-left corners are both\n        strongly contributing AND reliably associated with the class difference.\n        These are the best biomarker candidates.\n        \n        Returns:\n            p_cov  : 1D array \u2014 covariance (loadings)\n            p_corr : 1D array \u2014 correlation coefficients\n        \"\"\"\n        p = self.p_pred_  # Covariance (loadings)\n        \n        # Pearson correlation between each variable and the predictive score\n        t = self.t_pred_\n        X_filt = self.X_filtered_\n        pcorr = np.array([np.corrcoef(X_filt[:, j], t)[0, 1] for j in range(X_filt.shape[1])])\n        \n        return p, pcorr\n\n\nprint(\"\u2705 OPLS-DA class defined.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Select Classes and Fit OPLS-DA\n\nOPLS-DA works best for **pairwise** comparisons. Select two classes to compare below. If you have more than two classes, you can repeat this analysis for each pair."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# OPLS-DA: SELECT CLASSES AND FIT\n# ============================================================\n# Choose which two classes to compare. Change these to run different\n# pairwise comparisons (e.g., EVOO vs ROO, ROO vs Adulterated).\nCLASS_A = unique_classes[0]  # First class (coded as 0)\nCLASS_B = unique_classes[2]  # Second class (coded as 1)\nN_ORTHO = 1                  # Number of orthogonal components to remove\n\nprint(f\"Comparing: {CLASS_A} vs {CLASS_B}\")\n\n# --- Subset data to the two selected classes ---\nmask_ab = np.isin(class_labels, [CLASS_A, CLASS_B])\nX_ab = X_processed[mask_ab]\ny_ab = (class_labels[mask_ab] == CLASS_B).astype(float)  # Binary: 0 = CLASS_A, 1 = CLASS_B\nids_ab = sample_ids[mask_ab]\nlabels_ab = class_labels[mask_ab]\n\nprint(f\"Samples: {(y_ab==0).sum()} x {CLASS_A}, {(y_ab==1).sum()} x {CLASS_B}\")\n\n# --- Fit OPLS-DA ---\nopls = OPLSDA(n_ortho=N_ORTHO)\nopls.fit(X_ab, y_ab)\n\nprint(f\"\\nR\u00b2Y = {opls.R2Y_:.3f}\")\nprint(f\"Predictive score range: {opls.t_pred_.min():.3f} to {opls.t_pred_.max():.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# OPLS-DA SCORES PLOT: PREDICTIVE vs ORTHOGONAL\n# ============================================================\n# X-axis: predictive score (separates the two classes)\n# Y-axis: orthogonal score (within-class variation unrelated to class)\n#\n# Good models show clear horizontal separation between classes,\n# with the orthogonal axis capturing other structured variation.\n\nopls_df = pd.DataFrame({\n    \"Sample_ID\": ids_ab,\n    \"Class\": labels_ab,\n    \"t_predictive\": opls.t_pred_,\n    \"t_orthogonal\": opls.t_ortho_[:, 0]\n})\n\nfig = px.scatter(\n    opls_df, x=\"t_predictive\", y=\"t_orthogonal\",\n    color=\"Class\", hover_name=\"Sample_ID\",\n    title=f\"OPLS-DA Scores: {CLASS_A} vs {CLASS_B} (R\u00b2Y = {opls.R2Y_:.3f})\",\n    labels={\"t_predictive\": \"Predictive score (t[1])\", \"t_orthogonal\": \"Orthogonal score (t_ortho[1])\"},\n    color_discrete_sequence=px.colors.qualitative.Set2\n)\n# Vertical line at zero: samples on the left belong to CLASS_A, right to CLASS_B\nfig.add_vline(x=0, line_dash=\"dash\", line_color=\"grey\", opacity=0.5)\nfig.update_layout(template=\"plotly_white\", height=550, width=700)\nfig.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# S-PLOT: IDENTIFY RELIABLE BIOMARKERS\n# ============================================================\n# The S-plot combines two pieces of information for each variable:\n#   x-axis: p(cov)  = loading (magnitude of contribution to class difference)\n#   y-axis: p(corr) = correlation (reliability of the association)\n#\n# Variables in the CORNERS of the S-shape are both large in magnitude\n# AND highly reliable \u2014 these are the best biomarker candidates.\n# Upper-right: higher in CLASS_B; Lower-left: higher in CLASS_A\n\np_cov, p_corr = opls.s_plot_data()\n\nfig = go.Figure()\n\n# All variables as small points, coloured by correlation strength\nfig.add_trace(go.Scatter(\n    x=p_cov, y=p_corr,\n    mode=\"markers\",\n    marker=dict(\n        size=5,\n        color=np.abs(p_corr),           # Colour intensity = reliability\n        colorscale=\"RdYlBu_r\",\n        showscale=True,\n        colorbar=dict(title=\"|Correlation|\")\n    ),\n    hovertext=[f\"{v} cm\u207b\u00b9\" for v in variable_names],\n    hoverinfo=\"text\"\n))\n\n# --- Highlight significant variables ---\n# Criteria: |p(corr)| > 0.5 AND |p(cov)| in top 10th percentile\np_thresh = np.percentile(np.abs(p_cov), 90)\nsig_mask = (np.abs(p_corr) > 0.5) & (np.abs(p_cov) > p_thresh)\n\nif sig_mask.any():\n    fig.add_trace(go.Scatter(\n        x=p_cov[sig_mask], y=p_corr[sig_mask],\n        mode=\"markers+text\",\n        text=[f\"{v}\" for v in variable_names[sig_mask]],\n        textposition=\"top center\", textfont=dict(size=8),\n        marker=dict(size=10, color=\"red\", symbol=\"diamond\"),\n        name=\"Significant\", showlegend=True\n    ))\n\n# Reference lines at zero\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"grey\", opacity=0.3)\nfig.add_vline(x=0, line_dash=\"dash\", line_color=\"grey\", opacity=0.3)\n\nfig.update_layout(\n    title=f\"S-Plot: {CLASS_A} vs {CLASS_B}\",\n    xaxis_title=\"Covariance p(cov)\",\n    yaxis_title=\"Correlation p(corr)\",\n    template=\"plotly_white\",\n    height=550, width=700\n)\nfig.show()\n\n# --- Report significant variables ---\nprint(\"Variables in S-plot corners (reliable biomarkers):\")\nif sig_mask.any():\n    for idx in np.where(sig_mask)[0]:\n        direction = \"higher in \" + CLASS_B if p_cov[idx] > 0 else \"higher in \" + CLASS_A\n        print(f\"  {variable_names[idx]:>10s} cm\u207b\u00b9: p(cov)={p_cov[idx]:+.4f}, p(corr)={p_corr[idx]:+.3f} ({direction})\")\nelse:\n    print(\"  No variables met both thresholds. Try adjusting the criteria above.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Permutation Testing (Model Validation)\n\nPermutation testing is the gold standard for validating OPLS-DA models. The procedure:\n\n1. Randomly shuffle the class labels\n2. Fit a new OPLS-DA model on the shuffled data\n3. Repeat many times (e.g., 100\u2013200 permutations)\n4. Compare the real model's R\u00b2Y with the distribution of permuted R\u00b2Y values\n\nIf the real model is significantly better than the permuted models, the classification is genuine and not due to chance or overfitting.\n\n> **Note:** This can take a minute to run depending on the number of permutations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PERMUTATION TEST FOR OPLS-DA\n# ============================================================\n# This tests the null hypothesis: \"the OPLS-DA model performs no\n# better than chance\". We repeatedly shuffle class labels, fit\n# new models, and compare their R\u00b2Y to the real model.\n\nN_PERMUTATIONS = 100  # 100 for quick testing; increase to 200 for publication\n\nprint(f\"Running {N_PERMUTATIONS} permutations...\")\n\nperm_R2Y = []    # R\u00b2Y for each permuted model\nperm_corr = []   # Correlation between permuted and original y labels\n\nfor i in range(N_PERMUTATIONS):\n    # Randomly shuffle the class labels (breaks the real X-Y relationship)\n    y_perm = np.random.permutation(y_ab)\n    \n    # Track how correlated the permuted labels are with the original\n    # (permutations closer to the original will naturally give higher R\u00b2Y)\n    corr_with_original = np.abs(np.corrcoef(y_ab, y_perm)[0, 1])\n    \n    # Fit OPLS-DA on the permuted data\n    opls_perm = OPLSDA(n_ortho=N_ORTHO)\n    opls_perm.fit(X_ab, y_perm)\n    \n    perm_R2Y.append(opls_perm.R2Y_)\n    perm_corr.append(corr_with_original)\n    \n    if (i + 1) % 20 == 0:\n        print(f\"  {i + 1}/{N_PERMUTATIONS} done...\")\n\nperm_R2Y = np.array(perm_R2Y)\nperm_corr = np.array(perm_corr)\n\n# --- Permutation plot ---\n# Each grey dot is one permuted model. The red star is the real model.\n# If the real model is well above the permuted cloud, the model is valid.\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=perm_corr, y=perm_R2Y,\n    mode=\"markers\", name=\"Permuted\",\n    marker=dict(color=\"lightgrey\", size=6, line=dict(color=\"grey\", width=0.5))\n))\n\n# Real model at correlation = 1.0 (perfectly correlated with itself)\nfig.add_trace(go.Scatter(\n    x=[1.0], y=[opls.R2Y_],\n    mode=\"markers\", name=\"Real model\",\n    marker=dict(color=\"red\", size=14, symbol=\"star\")\n))\n\n# Trend line through permuted points (should intercept y-axis near 0)\nfit_coef = np.polyfit(perm_corr, perm_R2Y, 1)\nx_line = np.linspace(0, 1, 50)\nfig.add_trace(go.Scatter(\n    x=x_line, y=np.polyval(fit_coef, x_line),\n    mode=\"lines\", name=\"Trend\", line=dict(dash=\"dash\", color=\"grey\")\n))\n\nfig.update_layout(\n    title=f\"Permutation Test ({N_PERMUTATIONS} permutations)\",\n    xaxis_title=\"Correlation with original y\",\n    yaxis_title=\"R\u00b2Y\",\n    template=\"plotly_white\",\n    height=500, width=600\n)\nfig.show()\n\n# --- Statistical significance ---\n# p-value = proportion of permuted models with R\u00b2Y >= real R\u00b2Y\np_value = (perm_R2Y >= opls.R2Y_).sum() / N_PERMUTATIONS\nprint(f\"\\nReal model R\u00b2Y: {opls.R2Y_:.3f}\")\nprint(f\"Permuted R\u00b2Y: {perm_R2Y.mean():.3f} \u00b1 {perm_R2Y.std():.3f}\")\nprint(f\"Permutation p-value: {p_value:.3f}\")\nif p_value < 0.05:\n    print(\"\u2705 Model is statistically significant (p < 0.05). Classification is genuine.\")\nelse:\n    print(\"\u26a0\ufe0f Model is NOT significant. Consider overfitting or insufficient class differences.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n\nDownload key results as CSV files for your report."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# EXPORT KEY RESULTS\n# ============================================================\n# Save analysis results as CSV files that can be used in your report\n# or imported into other software (Excel, R, SIMCA, etc.)\n\n# --- PCA scores (first 5 components) ---\npca_export = pd.DataFrame(scores[:, :5], columns=[f\"PC{i+1}\" for i in range(5)])\npca_export.insert(0, \"Sample_ID\", sample_ids)\npca_export.insert(1, \"Class\", class_labels)\npca_export.to_csv(\"pca_scores.csv\", index=False)\n\n# --- PCA loadings (first 5 components) ---\nloadings_export = pd.DataFrame(loadings[:5].T, columns=[f\"PC{i+1}\" for i in range(5)])\nloadings_export.insert(0, \"Variable\", variable_names)\nloadings_export.to_csv(\"pca_loadings.csv\", index=False)\n\n# --- VIP scores ---\nvip_export = pd.DataFrame({\"Variable\": variable_names, \"VIP\": vip_scores})\nvip_export.to_csv(\"vip_scores.csv\", index=False)\n\nprint(\"\u2705 Files saved:\")\nprint(\"  - pca_scores.csv\")\nprint(\"  - pca_loadings.csv\")\nprint(\"  - vip_scores.csv\")\n\n# --- Download in Colab ---\n# The files.download() function triggers a browser download\ntry:\n    from google.colab import files\n    files.download(\"pca_scores.csv\")\n    files.download(\"pca_loadings.csv\")\n    files.download(\"vip_scores.csv\")\nexcept ImportError:\n    print(\"\\n(Not running in Colab \u2014 files saved to current directory)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n\nJolliffe, I.T. (2002). *Principal Component Analysis*, 2nd ed. Springer.\n\nBarker, M. & Rayens, W. (2003). Partial least squares for discrimination. *Journal of Chemometrics*, 17(3), 166\u2013173.\n\nTrygg, J. & Wold, S. (2002). Orthogonal projections to latent structures (O-PLS). *Journal of Chemometrics*, 16(3), 119\u2013128.\n\nWiklund, S. et al. (2008). Visualization of GC/TOF-MS-based metabolomics data for identification of biochemically interesting compounds using OPLS class models. *Analytical Chemistry*, 80(1), 115\u2013122.\n\nChong, I.G. & Jun, C.H. (2005). Performance of some variable selection methods when multicollinearity is present. *Chemometrics and Intelligent Laboratory Systems*, 78(1\u20132), 103\u2013112.\n\nEilers, P.H.C. & Boelens, H.F.M. (2005). Baseline correction with asymmetric least squares smoothing. Leiden University Medical Centre report.\n\nBarnes, R.J., Dhanoa, M.S. & Lister, S.J. (1989). Standard normal variate transformation and de-trending of near-infrared diffuse reflectance spectra. *Applied Spectroscopy*, 43(5), 772\u2013777.\n\n*Notebook prepared for FBMFOR \u2014 Food Fraud Analysis, University of Reading*"
   ]
  }
 ]
}